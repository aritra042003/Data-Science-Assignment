{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5VCX9ao65Fkn"
      },
      "outputs": [],
      "source": [
        "# Install the latest versions of TensorFlow and Keras\n",
        "!pip install --upgrade tensorflow\n",
        "!pip install --upgrade keras\n",
        "\n",
        "# Import libraries and print their versions\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "\n",
        "print(\"TensorFlow version:\", tf.__version__)\n",
        "print(\"Keras version:\", keras.__version__)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q2. Load the Wine Quality dataset and explore its dimensions."
      ],
      "metadata": {
        "id": "jlAJ7pId5rRT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Load the Wine Quality dataset\n",
        "  # Replace with the actual path to your dataset file\n",
        "wine_data = pd.read_csv(wine.csv)\n",
        "\n",
        "# Display the first few rows of the dataset\n",
        "print(\"First few rows of the Wine Quality dataset:\")\n",
        "print(wine_data.head())\n",
        "\n",
        "# Explore the dimensions of the dataset\n",
        "num_rows, num_columns = wine_data.shape\n",
        "print(f\"\\nNumber of rows: {num_rows}\")\n",
        "print(f\"Number of columns: {num_columns}\")\n"
      ],
      "metadata": {
        "id": "n9OvnFqo5s80"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# New Section\n",
        "\n",
        "Let's go through each step one by one:\n",
        "\n",
        "### Q3. Check for null values, identify categorical variables, and encode them.\n",
        "\n",
        "```python\n",
        "# Check for null values\n",
        "print(\"Null values in the dataset:\")\n",
        "print(wine_data.isnull().sum())\n",
        "\n",
        "# Identify categorical variables\n",
        "categorical_vars = wine_data.select_dtypes(include=['object']).columns\n",
        "print(\"\\nCategorical variables:\")\n",
        "print(categorical_vars)\n",
        "\n",
        "# Encode categorical variables\n",
        "wine_data_encoded = pd.get_dummies(wine_data, columns=categorical_vars, drop_first=True)\n",
        "```\n",
        "\n",
        "### Q4. Separate the features and target variables from the dataset.\n",
        "\n",
        "```python\n",
        "# Assuming the target variable is 'target_column_name', replace it with the actual target column name\n",
        "target_column_name = 'target_column_name'\n",
        "X = wine_data_encoded.drop(columns=[target_column_name])\n",
        "y = wine_data_encoded[target_column_name]\n",
        "```\n",
        "\n",
        "### Q5. Perform a train-test split, dividing the data into training, validation, and test datasets.\n",
        "\n",
        "```python\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "X_train, X_temp, y_train, y_temp = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.5, random_state=42)\n",
        "```\n",
        "\n",
        "### Q6. Scale the dataset using an appropriate scaling technique.\n",
        "\n",
        "```python\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_val_scaled = scaler.transform(X_val)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "```\n",
        "\n",
        "### Q7-Q9. Design and implement at least two hidden layers and an output layer for the binary categorical variables, create a Sequential model, add the layers, and print the summary.\n",
        "\n",
        "```python\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense\n",
        "\n",
        "# Design the model architecture\n",
        "model = Sequential()\n",
        "model.add(Dense(64, input_dim=X_train_scaled.shape[1], activation='relu'))\n",
        "model.add(Dense(32, activation='relu'))\n",
        "model.add(Dense(1, activation='sigmoid'))\n",
        "\n",
        "# Print the model summary\n",
        "model.summary()\n",
        "```\n",
        "\n",
        "### Q10-Q11. Set the loss function, optimizer, and include the accuracy metric in the model. Compile the model.\n",
        "\n",
        "```python\n",
        "# Compile the model\n",
        "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "```\n",
        "\n",
        "### Q12. Fit the model to the training data using appropriate batch size and number of epochs.\n",
        "\n",
        "```python\n",
        "history = model.fit(X_train_scaled, y_train, epochs=20, batch_size=32, validation_data=(X_val_scaled, y_val))\n",
        "```\n",
        "\n",
        "### Q13. Obtain the model's parameters (weights and biases).\n",
        "\n",
        "```python\n",
        "# Get model parameters\n",
        "weights_biases = model.get_weights()\n",
        "```\n",
        "\n",
        "### Q14. Store the model's training history as a Pandas DataFrame.\n",
        "\n",
        "```python\n",
        "# Store training history as DataFrame\n",
        "history_df = pd.DataFrame(history.history)\n",
        "```\n",
        "\n",
        "### Q15. Plot the training history (e.g., accuracy and loss) using suitable visualization techniques.\n",
        "\n",
        "```python\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Plot training history\n",
        "plt.plot(history_df['accuracy'], label='Train Accuracy')\n",
        "plt.plot(history_df['val_accuracy'], label='Validation Accuracy')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.legend()\n",
        "plt.show()\n",
        "\n",
        "plt.plot(history_df['loss'], label='Train Loss')\n",
        "plt.plot(history_df['val_loss'], label='Validation Loss')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend()\n",
        "plt.show()\n",
        "```\n",
        "\n",
        "### Q16. Evaluate the model's performance using the test dataset and report relevant metrics.\n",
        "\n",
        "```python\n",
        "# Evaluate model on the test dataset\n",
        "test_loss, test_accuracy = model.evaluate(X_test_scaled, y_test)\n",
        "print(f'Test Loss: {test_loss:.4f}')\n",
        "print(f'Test Accuracy: {test_accuracy:.4f}')\n",
        "```\n",
        "\n",
        "This set of code snippets covers each step from data preprocessing to model evaluation. Adjust parameters and architecture according to your specific requirements."
      ],
      "metadata": {
        "id": "TG7IeZ3F6k6A"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "SHen62nX6xFN"
      }
    }
  ]
}