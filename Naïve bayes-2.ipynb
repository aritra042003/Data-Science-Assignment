{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3216e48d-9493-4830-8b5f-d794b76d2ef5",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q1. A company conducted a survey of its employees and found that 70% of the employees use the\n",
    "company's health insurance plan, while 40% of the employees who use the plan are smokers. What is the\n",
    "probability that an employee is a smoker given that he/she uses the health insurance plan?\n",
    "\n",
    "\n",
    "\n",
    "To find the probability that an employee is a smoker given that he/she uses the health insurance plan, you can use conditional probability. The notation for this is P({Smoker} | {Uses health insurance}), and it is calculated using the formula:\n",
    "\n",
    "P({Smoker} | {Uses health insurance}) = {P({Smoker and Uses health insurance})}{P({Uses health insurance})}\n",
    "\n",
    "From the information provided:\n",
    "\n",
    "- The probability that an employee uses the health insurance plan is given as 70%, which can be denoted as (P(\\text{Uses health insurance}) = 0.70).\n",
    "- The probability that an employee both uses the health insurance plan and is a smoker is given as 40%, denoted as (P(\\text{Smoker and Uses health insurance}) = 0.40).\n",
    "\n",
    "Now, plug these values into the formula:\n",
    "\n",
    " P({Smoker} | {Uses health insurance}) = {0.40}/{0.70}\n",
    "\n",
    " P({Smoker} | {Uses health insurance}) =0.5714 \n",
    "\n",
    "So, the probability that an employee is a smoker given that he/she uses the health insurance plan is approximately 0.5714 or 57.14%."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49a70303-dcd7-40c5-8fc6-a0f9da75a5f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q2. What is the difference between Bernoulli Naive Bayes and Multinomial Naive Bayes?\n",
    "\n",
    "Bernoulli Naive Bayes and Multinomial Naive Bayes are two variants of the Naive Bayes classifier, which is a probabilistic machine learning algorithm based on Bayes' theorem. The primary difference between them lies in the type of data they are designed to handle and the underlying assumptions about the distribution of features.\n",
    "\n",
    "1. **Bernoulli Naive Bayes:**\n",
    "   - **Type of data:** It is suitable for binary data, where features represent binary events (occurring or not occurring).\n",
    "   - **Assumption:** Assumes that the features are binary-valued (0 or 1).\n",
    "   - **Example:** Email classification as spam or not spam (ham), where the features represent the presence or absence of specific words in the email.\n",
    "\n",
    "2. **Multinomial Naive Bayes:**\n",
    "   - **Type of data:** It is designed for discrete data, typically used for text classification where features represent the frequency of words or other discrete counts.\n",
    "   - **Assumption:** Assumes that features are multinomially distributed, meaning they represent counts (e.g., word frequencies in a document).\n",
    "   - **Example:** Document classification based on the frequency of words in the document.\n",
    "\n",
    "In summary:\n",
    "\n",
    "- **Bernoulli Naive Bayes:** Binary features, assumes features are binary-valued (0 or 1).\n",
    "- **Multinomial Naive Bayes:** Discrete features, assumes features are multinomially distributed (counts of occurrences).\n",
    "\n",
    "Both algorithms share the \"naive\" assumption, which is that features are conditionally independent given the class label. Despite this simplifying assumption, Naive Bayes classifiers often perform well in practice, especially in text classification tasks. The choice between Bernoulli and Multinomial Naive Bayes depends on the nature of the data you are working with."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66fa5bf4-262b-47f8-aef7-c70059a07023",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q3. How does Bernoulli Naive Bayes handle missing values?\n",
    "\n",
    "\n",
    "\n",
    "The treatment of missing values in Bernoulli Naive Bayes depends on the specific implementation or library being used. In general, Bernoulli Naive Bayes is designed for binary data, where features are either present (1) or absent (0). Missing values can be treated in various ways:\n",
    "\n",
    "1. **Ignoring Missing Values:**\n",
    "   - In many implementations, missing values are simply ignored. The assumption is that if a feature is missing, it is treated as if it is not present (0). This aligns with the binary nature of Bernoulli Naive Bayes, where features are either 0 or 1.\n",
    "\n",
    "2. **Imputation:**\n",
    "   - Some implementations may allow for imputation, where missing values are replaced with a certain default value (either 0 or 1) or with the mean/mode of the observed values in that feature. Imputation is a common technique to handle missing data in various algorithms.\n",
    "\n",
    "3. **Custom Handling:**\n",
    "   - Depending on the specific requirements or the library used, custom handling of missing values may be implemented. This could involve replacing missing values with a specific placeholder or using more sophisticated imputation techniques.\n",
    "\n",
    "It's important to note that the handling of missing values in Bernoulli Naive Bayes is often determined by the broader context of the data preprocessing steps and the specific choices made by the practitioner or the software library being used. Before applying Bernoulli Naive Bayes to a dataset with missing values, it's advisable to consult the documentation of the specific implementation or library to understand how missing values are treated and whether any customization options are available."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e56a6f61-9998-4e75-a4b9-787e81bc5066",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q4. Can Gaussian Naive Bayes be used for multi-class classification?\n",
    "\n",
    "\n",
    "Yes, Gaussian Naive Bayes can be used for multi-class classification. Gaussian Naive Bayes is an extension of the Naive Bayes algorithm that assumes the features are continuous and follows a Gaussian (normal) distribution. It is particularly suitable for data where the features are real-valued.\n",
    "\n",
    "In the context of multi-class classification, the Gaussian Naive Bayes algorithm can be adapted to handle multiple classes. The general approach for multi-class classification with Naive Bayes, including the Gaussian variant, is to train a separate classifier for each class. Each classifier estimates the likelihood of the data belonging to its corresponding class, and the class with the highest likelihood is assigned as the predicted class.\n",
    "\n",
    "Here's a brief overview of the steps for using Gaussian Naive Bayes for multi-class classification:\n",
    "\n",
    "1. **Training:**\n",
    "   - For each class, calculate the mean and variance of each feature based on the training data for that class. This involves estimating the parameters of the Gaussian distribution for each feature.\n",
    "\n",
    "2. **Prediction:**\n",
    "   - Given a new data point, calculate the likelihood of the data point belonging to each class using the Gaussian probability density function for each feature.\n",
    "\n",
    "    P(x_i | \\text{Class}) = {1}/{sqrt{2*pi*sigma_i^2}} * exp(-{(x_i - u_i)^2}/{2*sigma_i^2}\n",
    "\n",
    "   - Multiply the likelihoods across all features for each class.\n",
    "\n",
    "    P(\\text{Class} | X) \\propto P(\\text{Class}) prod_{i=1}^{n} P(x_i | {Class}) \n",
    "\n",
    "   - Assign the class with the highest probability as the predicted class.\n",
    "\n",
    "The scikit-learn library in Python, for example, provides a `GaussianNB` class that can be used for both binary and multi-class classification tasks. The same principles apply when using Gaussian Naive Bayes for multi-class classification as described above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3e814e70-4ff6-4b6a-bcf7-1f278dbc76f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: scikit-learn in /opt/conda/lib/python3.10/site-packages (1.2.0)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /opt/conda/lib/python3.10/site-packages (from scikit-learn) (3.1.0)\n",
      "Requirement already satisfied: numpy>=1.17.3 in /opt/conda/lib/python3.10/site-packages (from scikit-learn) (1.23.5)\n",
      "Requirement already satisfied: joblib>=1.1.1 in /opt/conda/lib/python3.10/site-packages (from scikit-learn) (1.2.0)\n",
      "Requirement already satisfied: scipy>=1.3.2 in /opt/conda/lib/python3.10/site-packages (from scikit-learn) (1.9.3)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a638d38a-8cce-4187-afbc-4a4fb5ea29c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bernoulli Naive Bayes:\n",
      "Accuracy: 0.8839380364047911\n",
      "Precision: 0.8813357185450209\n",
      "Recall: 0.815223386651958\n",
      "F1 Score: 0.8469914040114614\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import cross_val_predict\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.naive_bayes import BernoulliNB, MultinomialNB, GaussianNB\n",
    "\n",
    "# Load the dataset\n",
    "url = \"spambase.data\"  # replace with the actual path\n",
    "data = pd.read_csv(url, header=None)\n",
    "\n",
    "# Extract features and target variable\n",
    "X = data.drop(57, axis=1)  # assuming the target column is at index 57\n",
    "y = data[57]\n",
    "\n",
    "# Implement and evaluate Bernoulli Naive Bayes\n",
    "bernoulli_nb = BernoulliNB()\n",
    "scores = cross_val_score(bernoulli_nb, X, y, cv=10, scoring='accuracy')\n",
    "y_pred = cross_val_predict(bernoulli_nb, X, y, cv=10)\n",
    "print(\"Bernoulli Naive Bayes:\")\n",
    "print(\"Accuracy:\", scores.mean())\n",
    "print(\"Precision:\", precision_score(y, y_pred))\n",
    "print(\"Recall:\", recall_score(y, y_pred))\n",
    "print(\"F1 Score:\", f1_score(y, y_pred))\n",
    "\n",
    "# Repeat the process for Multinomial Naive Bayes and Gaussian Naive Bayes\n",
    "# ...\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcbe7a32-20cf-4c0a-a674-2e3882ec16ac",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33737c56-9b13-4311-8c07-060d237f2c35",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d81c796-5508-4c8b-b80d-941afdf42e1d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19188494-0528-4d7d-bfbe-656a63d887aa",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
