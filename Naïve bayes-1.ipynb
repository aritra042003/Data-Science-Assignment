{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d568a66-4954-455b-8f2f-799d92220cf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q1. What is Bayes' theorem? in short\n",
    "\n",
    "Bayes' theorem is a mathematical formula that describes the probability of an event based on prior knowledge of conditions that might be related to the event. It is named after Thomas Bayes, an 18th-century statistician and theologian. The theorem is expressed as:\n",
    "\n",
    " P(A|B) = {P(B|A) * P(A)}/ P(B)\n",
    "\n",
    "Where:\n",
    "- P(A|B)  is the probability of event A occurring given that event B has occurred.\n",
    "-  P(B|A)  is the probability of event B occurring given that event A has occurred.\n",
    "-  P(A)  is the probability of event A occurring.\n",
    "-  P(B)  is the probability of event B occurring.\n",
    "\n",
    "In essence, Bayes' theorem helps update our beliefs or probabilities about an event based on new evidence or information. It is widely used in statistics, machine learning, and various fields to make predictions and infer relationships between events."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9735e213-c383-4660-8b7b-67b7f1380983",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q2. What is the formula for Bayes' theorem?\n",
    "\n",
    "P(A|B) = {P(B|A) * P(A)}/ P(B)\n",
    "\n",
    "Where:\n",
    "- P(A|B)  is the probability of event A occurring given that event B has occurred.\n",
    "-  P(B|A)  is the probability of event B occurring given that event A has occurred.\n",
    "-  P(A)  is the probability of event A occurring.\n",
    "-  P(B)  is the probability of event B occurring.\n",
    "\n",
    "In essence, Bayes' theorem helps update our beliefs or probabilities about an event based on new evidence or information. It is widely used in statistics, machine learning, and various fields to make predictions and infer relationships between events."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3997914-9d3d-424c-9afd-cda0586f4852",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q3. How is Bayes' theorem used in practice?\n",
    "\n",
    "Bayes' theorem is used in various practical applications across different fields. Here are a few examples:\n",
    "\n",
    "1. **Medical Diagnosis:**\n",
    "   - Bayes' theorem is employed in medical diagnosis to update the probability of a particular disease given the presence of certain symptoms or test results.\n",
    "   - Doctors can use prior probabilities of diseases, the likelihood of symptoms given the presence or absence of a disease, and new test results to revise the probability of a patient having a specific condition.\n",
    "\n",
    "2. **Spam Filtering:**\n",
    "   - In email spam filtering, Bayes' theorem can be used to calculate the probability that an email is spam based on the occurrence of certain words or features.\n",
    "   - The spam filter updates its probability estimate as it encounters new emails, adapting to changing patterns and improving its accuracy over time.\n",
    "\n",
    "3. **Machine Learning and Classification:**\n",
    "   - Bayes' theorem is fundamental to Naive Bayes classifiers in machine learning. These classifiers assume independence between features, making calculations more manageable.\n",
    "   - It is used in text classification, sentiment analysis, and various other classification tasks.\n",
    "\n",
    "4. **Finance:**\n",
    "   - Bayes' theorem can be applied in finance for risk assessment and portfolio management. It helps update the probability of different financial events based on new information.\n",
    "   - Bayesian methods are also used in algorithmic trading strategies to make predictions about future market movements.\n",
    "\n",
    "5. **Quality Control:**\n",
    "   - In manufacturing, Bayes' theorem can be used for quality control by updating the probability of a defective product based on testing results.\n",
    "   - It helps in making decisions about whether to accept or reject a batch of products.\n",
    "\n",
    "6. **Search and Information Retrieval:**\n",
    "   - In information retrieval systems, Bayes' theorem is applied to estimate the relevance of documents to a user's query.\n",
    "   - It is used in search engines and recommendation systems to improve the accuracy of results.\n",
    "\n",
    "7. **Criminal Justice:**\n",
    "   - Bayes' theorem has been applied to forensic evidence analysis. It helps update the probability of guilt or innocence based on new pieces of evidence.\n",
    "\n",
    "In all these cases, Bayes' theorem provides a systematic and logical framework for updating probabilities as new evidence or information becomes available, making it a valuable tool for decision-making and inference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20a53657-dbec-4a6e-a2de-5791c439c9cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q4. What is the relationship between Bayes' theorem and conditional probability?\n",
    "\n",
    "Bayes' theorem and conditional probability are closely related concepts, and Bayes' theorem is essentially an extension of conditional probability. Let's explore the relationship between them.\n",
    "\n",
    "Conditional probability is the probability of an event occurring given that another event has already occurred. It is denoted by \\(P(A|B)\\), which represents the probability of event A occurring given that event B has occurred. Mathematically, it is defined as:\n",
    "\n",
    "   P(A|B) = P(A U B)/P(B)\n",
    "\n",
    "Here:\n",
    "-P(A U B) is the probability of both events A and B occurring.\n",
    "-P(B) is the probability of event B occurring.\n",
    "\n",
    "Now, Bayes' theorem provides a way to reverse the conditioning. It expresses the probability of event A given event B in terms of the probability of event B given event A. The formula for Bayes' theorem is:\n",
    "\n",
    " P(A|B) = {P(B|A) * P(A)}/{P(B)}\n",
    "\n",
    "In this formula:\n",
    "- P(A|B) is the probability of event A occurring given that event B has occurred.\n",
    "- P(B|A) is the probability of event B occurring given that event A has occurred.\n",
    "- P(A) is the prior probability of event A.\n",
    "- P(B) is the prior probability of event B.\n",
    "\n",
    "The connection between Bayes' theorem and conditional probability becomes apparent when you compare the two formulas. In Bayes' theorem, the term \\(P(B|A)\\) is a conditional probability, and it plays a crucial role in updating the probability of event A based on new evidence (event B).\n",
    "\n",
    "In summary, Bayes' theorem extends the concept of conditional probability by providing a systematic way to update probabilities when new information becomes available. It allows us to reverse the conditioning and calculate the probability of one event given another event, incorporating both prior knowledge and new evidence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a685f25e-9406-46f7-bdd6-7c7729256ad0",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q5.How do you choose which type of Naive Bayes classifier to use for any given problem?\n",
    "\n",
    "Choosing the right type of Naive Bayes classifier depends on the nature of your data and the assumptions you are willing to make about the independence of features. There are three main types of Naive Bayes classifiers: Gaussian Naive Bayes, Multinomial Naive Bayes, and Bernoulli Naive Bayes. Here's a brief overview of each and guidance on when to use them:\n",
    "\n",
    "1. **Gaussian Naive Bayes:**\n",
    "   - **Assumption:** Assumes that the features follow a normal distribution.\n",
    "   - **Use Case:** Suitable for continuous data where features are real-valued.\n",
    "   - **Example Applications:** Natural language processing tasks, such as text classification, where features may represent word frequencies.\n",
    "\n",
    "2. **Multinomial Naive Bayes:**\n",
    "   - **Assumption:** Assumes that features follow a multinomial distribution. It is commonly used for discrete data, such as word counts in document classification.\n",
    "   - **Use Case:** Well-suited for problems with features that describe discrete frequency counts (e.g., word counts in a document).\n",
    "   - **Example Applications:** Text classification, spam filtering, and other tasks involving discrete features.\n",
    "\n",
    "3. **Bernoulli Naive Bayes:**\n",
    "   - **Assumption:** Assumes that features are binary (e.g., presence or absence of a feature).\n",
    "   - **Use Case:** Appropriate for binary and sparse data, where features are either present or absent.\n",
    "   - **Example Applications:** Document classification, sentiment analysis, and other tasks involving binary features.\n",
    "\n",
    "### Choosing the Right Type:\n",
    "\n",
    "1. **Nature of Features:**\n",
    "   - **Continuous Features:** If your features are continuous and approximately follow a normal distribution, consider Gaussian Naive Bayes.\n",
    "   - **Discrete Features:** For discrete data, consider Multinomial or Bernoulli Naive Bayes based on the nature of your features.\n",
    "\n",
    "2. **Independence Assumption:**\n",
    "   - **Multinomial Naive Bayes:** When features represent counts or frequencies and are reasonably independent.\n",
    "   - **Bernoulli Naive Bayes:** When dealing with binary features, and the independence assumption is appropriate.\n",
    "   - **Gaussian Naive Bayes:** When dealing with continuous features, even if the independence assumption is not strictly met.\n",
    "\n",
    "3. **Sparsity of Data:**\n",
    "   - **Multinomial Naive Bayes:** Effective for datasets with sparse, high-dimensional feature vectors.\n",
    "   - **Bernoulli Naive Bayes:** Suitable for binary features and sparse datasets.\n",
    "\n",
    "4. **Domain Knowledge:**\n",
    "   - Consider your understanding of the problem and the characteristics of your data. Sometimes, it's beneficial to try multiple Naive Bayes classifiers and evaluate their performance on your specific dataset.\n",
    "\n",
    "Ultimately, the choice may involve experimentation and evaluation of different Naive Bayes models on your specific dataset. It's also worth noting that Naive Bayes classifiers are simple yet powerful, and the choice of the specific type might not be as critical as in some other more complex models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0618c68b-e5f7-45d6-bc46-ba549af417ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q6. Assignment:\n",
    "You have a dataset with two features, X1 and X2, and two possible classes, A and B. You want to use Naive\n",
    "Bayes to classify a new instance with features X1 = 3 and X2 = 4. The following table shows the frequency of\n",
    "each feature value for each class:\n",
    "Class X1=1 X1=2 X1=3 X2=1 X2=2 X2=3 X2=4\n",
    "A 3 3 4 4 3 3 3\n",
    "B 2 2 1 2 2 2 3\n",
    "Assuming equal prior probabilities for each class, which class would Naive Bayes predict the new instance\n",
    "to belong to?\n",
    "\n",
    "\n",
    "\n",
    "To classify a new instance using Naive Bayes, we need to calculate the likelihood and the prior probabilities for each class and then use Bayes' theorem to compute the posterior probabilities. In this case, we can assume equal prior probabilities for each class, meaning \\(P(A) = P(B) = 0.5\\).\n",
    "\n",
    "Let's denote the new instance features as X_1 = 3 and X_2 = 4, and the class labels as A and B.\n",
    "\n",
    "### Calculations:\n",
    "\n",
    "#### 1. Prior Probabilities:\n",
    " P(A) = P(B) = 0.5 \n",
    "\n",
    "#### 2. Likelihoods:\n",
    " P(X_1 = 3 | A) = {Frequency of  X_1 = 3  in Class A}/{Total instances in Class A} = 4/13\n",
    " P(X_1 = 3 | B) =  {Frequency of  X_1 = 3  in Class B}/{Total instances in Class B} = 1/7\n",
    "\n",
    " P(X_2 = 4 | A) = {Frequency of  X_2 = 4  in Class A}/{Total instances in Class A} = 3/13\n",
    " P(X_2 = 4 | B) = {Frequency of  X_2 = 4  in Class B}/{Total instances in Class B} = 3/7\n",
    "\n",
    "#### 3. Applying Bayes' Theorem:\n",
    " P(A | X_1 = 3, X_2 = 4) proportion P(A) * P(X_1 = 3 | A) * P(X_2 = 4 | A) \n",
    " P(B | X_1 = 3, X_2 = 4) proportion P(B) * P(X_1 = 3 | B) * P(X_2 = 4 | B) \n",
    "\n",
    "Since we are only comparing the probabilities between the two classes, we don't need to calculate the normalizing constant.\n",
    "\n",
    "### Results:\n",
    "\n",
    "Now, compare the proportional probabilities and choose the class with the higher probability.\n",
    "\n",
    " P(A | X_1 = 3, X_2 = 4) proportion 0.5 * {4}{13} * {3}{13} \n",
    " P(B | X_1 = 3, X_2 = 4) proportion 0.5 * {1}{7}  * {3}{7} \n",
    "\n",
    "Compare the proportional probabilities and choose the class with the higher probability.\n",
    "\n",
    " P(A | X_1 = 3, X_2 = 4) > P(B | X_1 = 3, X_2 = 4) \n",
    "\n",
    "Therefore, Naive Bayes would predict that the new instance belongs to Class A."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cda29f3-ccff-4e89-a593-b843c225caf7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
