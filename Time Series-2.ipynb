{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5211cc94-72ec-4f52-85e9-d57c372f33a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q1. What is meant by time-dependent seasonal components?\n",
    "\n",
    "\n",
    "\n",
    "Time-dependent seasonal components in a time series refer to seasonal patterns or variations that change over time. Seasonality is a recurring pattern that occurs at regular intervals, and it is often associated with external factors such as weather, holidays, or events. When these seasonal patterns vary or evolve over time, they are considered time-dependent.\n",
    "\n",
    "In a typical time series, seasonality may exhibit a consistent pattern, such as a regular increase in sales every December due to holiday shopping. However, in the presence of time-dependent seasonal components, the characteristics of this seasonality may change over the observed time period. For example:\n",
    "\n",
    "1. **Changing Amplitude:**\n",
    "   - The strength or amplitude of the seasonal effect may vary from year to year. For instance, the increase in sales during the holiday season might be more pronounced in some years and less prominent in others.\n",
    "\n",
    "2. **Shifting Peaks or Troughs:**\n",
    "   - The timing of peak or trough periods within a season may shift over time. For example, if a particular region experiences a delay in the onset of winter, the peak demand for heating may also be delayed compared to previous years.\n",
    "\n",
    "3. **Altering Seasonal Patterns:**\n",
    "   - The overall shape or form of the seasonal pattern may change. This could involve variations in the duration or intensity of the seasonality.\n",
    "\n",
    "Time-dependent seasonal components can pose challenges for time series modeling because traditional approaches often assume a constant and predictable seasonal pattern. However, ignoring or mischaracterizing time-dependent seasonality can lead to inaccurate forecasts and models that don't capture the true dynamics of the data.\n",
    "\n",
    "When dealing with time-dependent seasonal components, analysts may need to employ more advanced modeling techniques or adapt existing models to accommodate the evolving nature of seasonality. Flexible models that can automatically adjust to changing patterns over time, or methods that allow for dynamic updating of seasonal parameters, may be more suitable in such scenarios."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdaf0458-d106-44f2-9d0e-0af3e852c036",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q2. How can time-dependent seasonal components be identified in time series data?\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "Identifying time-dependent seasonal components in time series data involves recognizing patterns or variations in seasonality that change over time. Here are several approaches and techniques that can help identify time-dependent seasonal components:\n",
    "\n",
    "1. **Visual Inspection:**\n",
    "   - Plot the time series data and visually inspect for recurring patterns. Look for variations in the amplitude, timing, or shape of the seasonal peaks and troughs over different time periods.\n",
    "\n",
    "2. **Seasonal Subseries Plots:**\n",
    "   - Create seasonal subseries plots, which involve plotting subsets of the time series data corresponding to each season. This can reveal variations in seasonality across different seasons and years.\n",
    "\n",
    "3. **Boxplots or Violin Plots:**\n",
    "   - Use boxplots or violin plots to visualize the distribution of observations within each season over different time periods. Changes in the spread or central tendency can indicate variations in seasonality.\n",
    "\n",
    "4. **Moving Averages:**\n",
    "   - Calculate moving averages or rolling averages to smooth out short-term fluctuations and highlight longer-term patterns. Observe how the moving averages change over time to detect variations in seasonality.\n",
    "\n",
    "5. **Autocorrelation Function (ACF) and Partial Autocorrelation Function (PACF):**\n",
    "   - Analyze the ACF and PACF plots to identify the presence and characteristics of autocorrelation in the time series. Changes in autocorrelation patterns may indicate variations in seasonality.\n",
    "\n",
    "6. **Decomposition Techniques:**\n",
    "   - Apply decomposition methods such as Seasonal-Trend decomposition using LOESS (STL) or X-12-ARIMA to separate a time series into its trend, seasonal, and residual components. Observe how the seasonal component changes over different time periods.\n",
    "\n",
    "7. **Statistical Tests:**\n",
    "   - Conduct statistical tests for seasonality, such as the Augmented Dickey-Fuller (ADF) test or the Kwiatkowski-Phillips-Schmidt-Shin (KPSS) test. These tests can help assess the stationarity of the seasonal component.\n",
    "\n",
    "8. **Machine Learning Models:**\n",
    "   - Train machine learning models that are capable of capturing time-dependent patterns, such as recurrent neural networks (RNNs) or long short-term memory (LSTM) networks. These models can learn complex temporal relationships in the data.\n",
    "\n",
    "9. **Time Series Clustering:**\n",
    "   - Apply clustering techniques to identify groups of time series with similar seasonal patterns. Changes in cluster assignments over time may indicate variations in time-dependent seasonality.\n",
    "\n",
    "10. **Cross-Validation:**\n",
    "    - Perform cross-validation by training models on different subsets of the time series data to see how well the models generalize to unseen periods. A degradation in performance over time may signal changes in seasonality.\n",
    "\n",
    "It's important to note that identifying time-dependent seasonal components is often an iterative process that involves testing and refining different methods. Additionally, domain knowledge about the data and the factors influencing seasonality can provide valuable insights into the interpretation of the identified patterns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b7a800e-4e44-4e85-8e93-ef49f1c712f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q3. What are the factors that can influence time-dependent seasonal components?\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "Several factors can influence time-dependent seasonal components in a time series. These factors contribute to variations in the observed seasonal patterns over different time periods. Understanding these influences is crucial for accurately modeling and forecasting time-dependent seasonality. Here are some key factors:\n",
    "\n",
    "1. **Economic Factors:**\n",
    "   - Economic conditions, such as recessions, expansions, or changes in consumer spending habits, can influence seasonal patterns. For example, during economic downturns, the amplitude of seasonal peaks in retail sales may decrease.\n",
    "\n",
    "2. **Cultural and Religious Events:**\n",
    "   - Cultural or religious events that affect consumer behavior, such as holidays and festivals, can impact seasonal patterns. The timing and intensity of seasonal peaks may vary based on the calendar dates of these events.\n",
    "\n",
    "3. **Weather and Climate:**\n",
    "   - Seasonal weather variations can significantly influence the demand for certain products or services. For instance, the demand for heating in the winter or air conditioning in the summer can be influenced by variations in temperature and climate conditions.\n",
    "\n",
    "4. **Industry-Specific Factors:**\n",
    "   - Industry-specific factors, such as product launches, promotional activities, or changes in supply chain dynamics, can affect seasonal patterns. An industry undergoing rapid changes may experience evolving seasonalities.\n",
    "\n",
    "5. **Regulatory Changes:**\n",
    "   - Changes in regulations or policies can impact seasonal patterns. For example, alterations in trading hours, tax policies, or import/export regulations may influence the timing and intensity of seasonal peaks.\n",
    "\n",
    "6. **Technological Changes:**\n",
    "   - Advancements in technology can alter consumer behavior and impact seasonal patterns. For instance, the rise of e-commerce has influenced the timing and nature of seasonal shopping patterns.\n",
    "\n",
    "7. **Global Events:**\n",
    "   - Global events, such as pandemics, geopolitical events, or economic crises, can have widespread effects on consumer behavior and influence seasonal patterns across various industries.\n",
    "\n",
    "8. **Demographic Changes:**\n",
    "   - Changes in demographics, such as shifts in population size, age distribution, or lifestyle preferences, can influence when and how consumers engage in seasonal activities.\n",
    "\n",
    "9. **Supply Chain Dynamics:**\n",
    "   - Disruptions or changes in the supply chain, such as shortages, transportation issues, or changes in inventory management practices, can affect the availability of products and influence seasonal demand patterns.\n",
    "\n",
    "10. **Competitive Factors:**\n",
    "    - Competitive dynamics within an industry can impact seasonal patterns. For example, aggressive marketing campaigns or pricing strategies by competitors may influence consumer purchasing behavior during specific seasons.\n",
    "\n",
    "11. **Social Trends:**\n",
    "    - Social trends and cultural shifts can influence consumer preferences and behaviors. Changes in societal attitudes towards certain products or activities may result in evolving seasonal patterns.\n",
    "\n",
    "12. **Natural Events:**\n",
    "    - Natural events, such as hurricanes, earthquakes, or other disasters, can disrupt normal seasonal patterns by affecting supply chains, transportation, and consumer behavior.\n",
    "\n",
    "Understanding the interplay of these factors is essential for effectively modeling time-dependent seasonal components. Seasonal patterns may evolve over time due to a combination of these influences, and incorporating this knowledge into time series models allows for more accurate predictions and insights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c68ae0b1-3d65-4e71-ab36-3a73419f6fe6",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q4. How are autoregression models used in time series analysis and forecasting?\n",
    "\n",
    "\n",
    "\n",
    "Autoregression models are a type of time series model that leverages the relationship between an observation and its past observations to make predictions about future values. Autoregressive models are denoted as AR(p), where \"p\" represents the order of the autoregressive model. The order \"p\" indicates the number of lagged observations included in the model.\n",
    "\n",
    "The general form of an autoregressive model of order \\(p\\) (AR(p)) is expressed as:\n",
    "\n",
    "\\[ Y_t = c + \\phi_1 Y_{t-1} + \\phi_2 Y_{t-2} + \\ldots + \\phi_p Y_{t-p} + \\varepsilon_t \\]\n",
    "\n",
    "where:\n",
    "- \\( Y_t \\) is the value of the time series at time \\(t\\).\n",
    "- \\( c \\) is a constant.\n",
    "- \\( \\phi_1, \\phi_2, \\ldots, \\phi_p \\) are autoregressive parameters.\n",
    "- \\( \\varepsilon_t \\) is a white noise error term.\n",
    "\n",
    "### Steps in Using Autoregressive Models for Time Series Analysis and Forecasting:\n",
    "\n",
    "1. **Data Preparation:**\n",
    "   - Ensure the time series data is stationary. If not, apply differencing to remove trends and seasonality.\n",
    "\n",
    "2. **Model Identification:**\n",
    "   - Analyze the Autocorrelation Function (ACF) and Partial Autocorrelation Function (PACF) plots to identify the order (\\(p\\)) of the autoregressive model.\n",
    "   - The ACF plot shows the correlation of the time series with its past values at different lags.\n",
    "   - The PACF plot helps identify the direct relationship between the time series and its past values after accounting for intervening lags.\n",
    "\n",
    "3. **Parameter Estimation:**\n",
    "   - Estimate the autoregressive parameters (\\( \\phi_1, \\phi_2, \\ldots, \\phi_p \\)) using methods like the method of moments, maximum likelihood estimation, or other estimation techniques.\n",
    "\n",
    "4. **Model Fitting:**\n",
    "   - Fit the autoregressive model to the training data using the identified parameters.\n",
    "\n",
    "5. **Model Evaluation:**\n",
    "   - Evaluate the model's performance using statistical metrics such as Mean Squared Error (MSE), Root Mean Squared Error (RMSE), or others.\n",
    "   - Validate the model using a separate test dataset.\n",
    "\n",
    "6. **Forecasting:**\n",
    "   - Use the fitted autoregressive model to make future forecasts.\n",
    "\n",
    "7. **Model Refinement:**\n",
    "   - Refine the model by adjusting parameters based on performance on validation data.\n",
    "\n",
    "8. **Final Forecasting:**\n",
    "   - Make final forecasts on unseen data using the tuned autoregressive model.\n",
    "\n",
    "### Advantages of Autoregressive Models:\n",
    "\n",
    "- **Interpretability:**\n",
    "  - Autoregressive models are relatively interpretable, as they capture the direct relationship between a variable and its past values.\n",
    "\n",
    "- **Simplicity:**\n",
    "  - AR models are simple and computationally efficient compared to more complex models.\n",
    "\n",
    "### Limitations of Autoregressive Models:\n",
    "\n",
    "- **Assumption of Linearity:**\n",
    "  - AR models assume a linear relationship between the current value and its past values, which may not capture non-linear patterns.\n",
    "\n",
    "- **Sensitivity to Outliers:**\n",
    "  - Autoregressive models can be sensitive to outliers and may not perform well in the presence of extreme values.\n",
    "\n",
    "- **Limited Handling of Seasonality:**\n",
    "  - AR models may struggle to capture seasonal patterns or trends with long-term dependencies.\n",
    "\n",
    "- **Model Selection:**\n",
    "  - Selecting the appropriate order (\\(p\\)) for the autoregressive model can be challenging and often involves trial and error.\n",
    "\n",
    "Autoregressive models serve as a foundational component in more complex time series models, and they are valuable for capturing short-term dependencies in the data. However, in practice, analysts may use them in conjunction with other models to improve forecasting accuracy, especially when dealing with diverse and dynamic time series data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "785c3349-ac30-4ced-a247-fe22c24f65db",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q5. How do you use autoregression models to make predictions for future time points?\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "Autoregressive models use the relationship between an observation and its past observations to make predictions for future time points. Once the autoregressive model is identified and fitted to the historical data, you can follow these steps to make predictions for future time points:\n",
    "\n",
    "### Steps to Use Autoregressive Models for Predictions:\n",
    "\n",
    "1. **Model Identification:**\n",
    "   - Identify the order (\\(p\\)) of the autoregressive model (AR(p)) using tools like Autocorrelation Function (ACF) and Partial Autocorrelation Function (PACF) plots.\n",
    "\n",
    "2. **Parameter Estimation:**\n",
    "   - Estimate the autoregressive parameters (\\(\\phi_1, \\phi_2, \\ldots, \\phi_p\\)) using methods like the method of moments, maximum likelihood estimation, or other estimation techniques.\n",
    "\n",
    "3. **Model Fitting:**\n",
    "   - Fit the autoregressive model to the training data using the identified parameters. This involves training the model on a historical dataset.\n",
    "\n",
    "4. **Future Time Point Prediction:**\n",
    "   - Once the model is fitted, you can use it to make predictions for future time points.\n",
    "\n",
    "   - The formula for predicting the value at time \\(t\\) using an AR(p) model is:\n",
    "     \\[ \\hat{Y}_t = c + \\phi_1 Y_{t-1} + \\phi_2 Y_{t-2} + \\ldots + \\phi_p Y_{t-p} \\]\n",
    "\n",
    "   - \\(\\hat{Y}_t\\) is the predicted value at time \\(t\\).\n",
    "   - \\(c\\) is a constant.\n",
    "   - \\(\\phi_1, \\phi_2, \\ldots, \\phi_p\\) are the estimated autoregressive parameters.\n",
    "   - \\(Y_{t-1}, Y_{t-2}, \\ldots, Y_{t-p}\\) are the observed values at past time points.\n",
    "\n",
    "5. **Iterative Forecasting:**\n",
    "   - To make predictions for multiple future time points, the model can be applied iteratively. After predicting the value at time \\(t+1\\), it can be used as an input for predicting the value at time \\(t+2\\), and so on.\n",
    "\n",
    "6. **Validation and Refinement:**\n",
    "   - Evaluate the model's predictions using validation data or a separate test dataset. Compare the predicted values to the actual values to assess the model's accuracy.\n",
    "\n",
    "7. **Refinement (Optional):**\n",
    "   - If necessary, refine the model by adjusting parameters based on its performance on validation data.\n",
    "\n",
    "8. **Final Forecasting:**\n",
    "   - Make final forecasts for future time points on unseen data using the tuned autoregressive model.\n",
    "\n",
    "### Example:\n",
    "Suppose you have an AR(2) model, and you want to predict the value at time \\(t+1\\). The formula would be:\n",
    "\\[ \\hat{Y}_{t+1} = c + \\phi_1 Y_t + \\phi_2 Y_{t-1} \\]\n",
    "\n",
    "Here, \\(\\hat{Y}_{t+1}\\) is the predicted value at time \\(t+1\\), \\(c\\) is the constant, \\(\\phi_1\\) and \\(\\phi_2\\) are the estimated autoregressive parameters, and \\(Y_t\\) and \\(Y_{t-1}\\) are the observed values at times \\(t\\) and \\(t-1\\), respectively.\n",
    "\n",
    "It's important to note that the accuracy of predictions depends on the appropriateness of the chosen model order (\\(p\\)) and the quality of the estimated parameters. Additionally, using the model for forecasting beyond the observed data requires careful validation to ensure reliable predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fec1c60-b09a-4da6-afd5-fdf324c2e60a",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q6. What is a moving average (MA) model and how does it differ from other time series models?\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "A Moving Average (MA) model is a type of time series model that captures the relationship between an observation and a linear combination of past white noise error terms (residuals). Unlike autoregressive models that rely on past observations, MA models emphasize the influence of past errors.\n",
    "\n",
    "The general form of an MA(q) model is expressed as:\n",
    "\n",
    "\\[ Y_t = \\mu + \\varepsilon_t + \\theta_1 \\varepsilon_{t-1} + \\theta_2 \\varepsilon_{t-2} + \\ldots + \\theta_q \\varepsilon_{t-q} \\]\n",
    "\n",
    "where:\n",
    "- \\( Y_t \\) is the value of the time series at time \\(t\\).\n",
    "- \\( \\mu \\) is the mean of the time series.\n",
    "- \\( \\varepsilon_t \\) is the white noise error term at time \\(t\\).\n",
    "- \\( \\theta_1, \\theta_2, \\ldots, \\theta_q \\) are the moving average parameters.\n",
    "- \\( q \\) is the order of the MA model, representing the number of lagged error terms included.\n",
    "\n",
    "### Key Characteristics and Differences from Other Time Series Models:\n",
    "\n",
    "1. **Emphasis on Past Errors:**\n",
    "   - MA models emphasize the influence of past white noise errors rather than past values of the time series itself.\n",
    "\n",
    "2. **Absence of Autoregressive Components:**\n",
    "   - Unlike autoregressive models (AR models), MA models do not include lagged values of the time series as predictors. The focus is on modeling the impact of past errors.\n",
    "\n",
    "3. **Finite Memory:**\n",
    "   - MA models have finite memory. The impact of past errors diminishes as the lag increases, and beyond the order \\(q\\), the influence becomes negligible.\n",
    "\n",
    "4. **Stationary Time Series:**\n",
    "   - MA models are typically applied to stationary time series data. They may require differencing to achieve stationarity.\n",
    "\n",
    "5. **Model Order \\(q\\):**\n",
    "   - The order \\(q\\) represents the number of lagged error terms included in the model. Selecting an appropriate order is crucial for capturing the relevant information in the data.\n",
    "\n",
    "6. **Combination with AR Models:**\n",
    "   - In practice, moving average models are often used in combination with autoregressive models to form Autoregressive Moving Average (ARMA) models or with differencing to form Autoregressive Integrated Moving Average (ARIMA) models.\n",
    "\n",
    "7. **Model Identification:**\n",
    "   - Model identification involves analyzing the Autocorrelation Function (ACF) and Partial Autocorrelation Function (PACF) plots to determine the order (\\(q\\)) of the MA model.\n",
    "\n",
    "8. **Forecasting:**\n",
    "   - MA models are used for short-term forecasting and capturing short-term dependencies in the data. They can be applied to predict future values based on the estimated parameters.\n",
    "\n",
    "### Example:\n",
    "A simple example of an MA(1) model is given by:\n",
    "\\[ Y_t = \\mu + \\varepsilon_t + \\theta_1 \\varepsilon_{t-1} \\]\n",
    "\n",
    "Here, \\( \\theta_1 \\) is the moving average parameter, and \\( \\varepsilon_t \\) and \\( \\varepsilon_{t-1} \\) are the white noise error terms at times \\(t\\) and \\(t-1\\), respectively.\n",
    "\n",
    "In summary, Moving Average models provide a framework for modeling short-term dependencies in time series data based on the influence of past white noise errors. They are part of the broader family of ARIMA models commonly used for time series analysis and forecasting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "349335b6-112f-4f77-92cb-bea2ca29a7f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q7. What is a mixed ARMA model and how does it differ from an AR or MA model?\n",
    "\n",
    "\n",
    "\n",
    "A mixed Autoregressive Moving Average (ARMA) model is a combination of autoregressive (AR) and moving average (MA) components within the same time series model. This model is denoted as ARMA(p, q), where \"p\" represents the order of the autoregressive component and \"q\" represents the order of the moving average component.\n",
    "\n",
    "The general form of an ARMA(p, q) model is expressed as:\n",
    "\n",
    "\\[ Y_t = c + \\phi_1 Y_{t-1} + \\phi_2 Y_{t-2} + \\ldots + \\phi_p Y_{t-p} + \\varepsilon_t + \\theta_1 \\varepsilon_{t-1} + \\theta_2 \\varepsilon_{t-2} + \\ldots + \\theta_q \\varepsilon_{t-q} \\]\n",
    "\n",
    "where:\n",
    "- \\( Y_t \\) is the value of the time series at time \\(t\\).\n",
    "- \\( c \\) is a constant.\n",
    "- \\( \\phi_1, \\phi_2, \\ldots, \\phi_p \\) are the autoregressive parameters.\n",
    "- \\( \\varepsilon_t \\) is the white noise error term at time \\(t\\).\n",
    "- \\( \\theta_1, \\theta_2, \\ldots, \\theta_q \\) are the moving average parameters.\n",
    "- \\( p \\) is the order of the autoregressive component.\n",
    "- \\( q \\) is the order of the moving average component.\n",
    "\n",
    "### Key Characteristics and Differences from AR and MA Models:\n",
    "\n",
    "1. **Combination of AR and MA Components:**\n",
    "   - ARMA models combine both autoregressive and moving average components in a single model. This allows for capturing both the linear relationship with past values and the influence of past white noise errors.\n",
    "\n",
    "2. **Flexibility in Modeling Time Series:**\n",
    "   - ARMA models provide flexibility in capturing different aspects of time series data by incorporating both short-term dependencies (AR) and the impact of past errors (MA).\n",
    "\n",
    "3. **Model Order \\(p\\) and \\(q\\):**\n",
    "   - The orders \\(p\\) and \\(q\\) need to be determined based on the characteristics of the time series. Model identification involves analyzing Autocorrelation Function (ACF) and Partial Autocorrelation Function (PACF) plots.\n",
    "\n",
    "4. **ARMA vs. ARIMA:**\n",
    "   - ARMA models are a subset of Autoregressive Integrated Moving Average (ARIMA) models. ARIMA models also include differencing to achieve stationarity. If differencing is applied, the model becomes ARIMA(p, d, q), where \"d\" is the order of differencing.\n",
    "\n",
    "5. **Stationarity:**\n",
    "   - Like AR and MA models, ARMA models are typically applied to stationary time series data. Differencing may be required to achieve stationarity if the data is non-stationary.\n",
    "\n",
    "6. **Forecasting:**\n",
    "   - ARMA models are used for short-term forecasting and capturing both autocorrelation and short-term dependencies in the data.\n",
    "\n",
    "### Example:\n",
    "A simple example of an ARMA(1, 1) model is given by:\n",
    "\\[ Y_t = c + \\phi_1 Y_{t-1} + \\varepsilon_t + \\theta_1 \\varepsilon_{t-1} \\]\n",
    "\n",
    "Here, \\( \\phi_1 \\) is the autoregressive parameter, \\( \\varepsilon_t \\) is the white noise error term at time \\(t\\), \\( \\theta_1 \\) is the moving average parameter, and \\( \\varepsilon_{t-1} \\) is the white noise error term at time \\(t-1\\).\n",
    "\n",
    "In summary, ARMA models offer a flexible approach to time series modeling by combining autoregressive and moving average components. The choice of model order (\\(p, q\\)) is essential for accurately capturing the patterns in the time series data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92f36915-8c1c-4fe1-8d95-7cda4e9d856f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26a27985-002b-4204-b287-548b5b371daf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd9cfdfe-719e-46d1-8774-7d6ebc23e98a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07caa308-186a-4bfe-836f-d45cace1cdcf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c34d8e36-d5dc-4e36-a5f3-a328c3a5d7bd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70942ba3-b8b6-4db4-8183-0177fade19ab",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
