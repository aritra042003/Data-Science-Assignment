{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "563eb4ac-aa69-451a-992e-c91198a70ca9",
   "metadata": {},
   "outputs": [],
   "source": [
    "Part 1: Understanding Weight Initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2c042e7-908e-4148-836e-722cb787ec12",
   "metadata": {},
   "outputs": [],
   "source": [
    "1. Explain the importance of weight initialization in artificial neural networks. Why is it necessary to initialize\n",
    "the weights carefully?\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "Weight initialization is a crucial aspect of training artificial neural networks (ANNs) because it greatly influences the learning process and the performance of the model. Here are several reasons why careful weight initialization is essential:\n",
    "\n",
    "1. **Avoiding Vanishing or Exploding Gradients:**\n",
    "   - During backpropagation, gradients are used to update the weights of the network. If the weights are initialized too small, the gradients may become vanishingly small as they propagate through the layers, leading to slow or stalled learning.\n",
    "   - Conversely, if the weights are initialized too large, the gradients may explode, causing the model to diverge during training. This instability can make it difficult for the network to converge to a good solution.\n",
    "\n",
    "2. **Facilitating Convergence:**\n",
    "   - Proper weight initialization helps the network converge faster during training. When the weights are initialized in a way that encourages the flow of information through the network, the optimization algorithm can more effectively update the weights to minimize the loss.\n",
    "\n",
    "3. **Breaking Symmetry:**\n",
    "   - Symmetry in weight initialization can lead to symmetry in learning, where neurons in the same layer learn the same features. This redundancy hinders the network's capacity to learn diverse and useful representations. Careful weight initialization helps break this symmetry, promoting the development of diverse features in the network.\n",
    "\n",
    "4. **Improving Generalization:**\n",
    "   - Well-initialized weights contribute to better generalization, meaning that the network can perform well on unseen data. Carefully chosen initial weights provide a starting point that is conducive to learning meaningful patterns in the data, which can lead to improved performance on both the training and validation sets.\n",
    "\n",
    "5. **Enhancing Robustness:**\n",
    "   - Sensible weight initialization can improve the robustness of the model to variations in the input data. It helps the model adapt more effectively to different examples and prevents it from being overly sensitive to small changes in the input.\n",
    "\n",
    "Common weight initialization techniques include:\n",
    "   - **Zero Initialization:** Setting all weights to zero. This is generally not recommended as it can lead to symmetry issues.\n",
    "   - **Random Initialization:** Setting weights to small random values. Common methods include Gaussian initialization and uniform initialization.\n",
    "   - **Xavier/Glorot Initialization:** A method designed to balance the scale of weights in such a way that the variance of the input and output of each layer remains roughly the same.\n",
    "\n",
    "In summary, careful weight initialization is critical for addressing numerical stability issues, promoting efficient learning, breaking symmetries, and ultimately improving the performance and generalization of artificial neural networks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ef98ea0-e6c7-43de-8c7c-f7dddf8710fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "2. Describe the challenges associated with improper weight initialization. How do these issues affect model\n",
    "training and convergence?\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "Improper weight initialization can introduce various challenges during the training of artificial neural networks, negatively impacting the convergence and performance of the model. Here are some of the challenges associated with improper weight initialization:\n",
    "\n",
    "1. **Vanishing Gradients:**\n",
    "   - If weights are initialized to small values, especially close to zero, the gradients during backpropagation may become very small as they are propagated backward through the layers. This is known as the vanishing gradient problem. As a result, the weights of the earlier layers receive negligible updates, impeding the learning process in those layers.\n",
    "\n",
    "2. **Exploding Gradients:**\n",
    "   - Conversely, if weights are initialized to large values, the gradients can become extremely large during backpropagation, leading to exploding gradients. This can cause the weights to be updated by excessively large amounts, potentially causing the model to diverge or oscillate during training.\n",
    "\n",
    "3. **Symmetry Issues:**\n",
    "   - If all the weights in a layer are initialized to the same value (e.g., zero), the neurons in that layer will have identical gradients during backpropagation. This symmetry in learning can result in neurons in the same layer learning the same features, limiting the expressive power of the network.\n",
    "\n",
    "4. **Slow Convergence:**\n",
    "   - Poor weight initialization can lead to slow convergence during training. If the initial weights are not set properly, the optimization algorithm may take longer to find a suitable set of weights that minimizes the loss function.\n",
    "\n",
    "5. **Stuck Neurons:**\n",
    "   - Neurons may become \"stuck\" or non-responsive if their weights are initialized in a way that prevents them from learning meaningful representations from the input data. This can occur if weights are too small, preventing neurons from being effectively updated.\n",
    "\n",
    "6. **Training Instability:**\n",
    "   - The overall training process can become unstable if the weights are not initialized carefully. Training instability can manifest as erratic changes in the loss function, making it difficult for the model to converge to a desired solution.\n",
    "\n",
    "7. **Poor Generalization:**\n",
    "   - Improper weight initialization can lead to poor generalization, where the model performs well on the training data but fails to generalize to unseen data. This is because the model may have learned spurious patterns in the training data that do not generalize well.\n",
    "\n",
    "To address these challenges, it's important to use appropriate weight initialization techniques, such as Xavier/Glorot initialization or He initialization, which aim to set the initial weights in a way that mitigates the vanishing/exploding gradient problems, breaks symmetry, and promotes effective learning throughout the network. Choosing the right initialization strategy is a crucial step in building robust and well-performing neural networks.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7dc3aab-f26d-414c-9294-e43597efa4f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "3. Discuss the concept of variance and how it relates to weight initialization. Why is it crucial to consider the\n",
    "variance of weights during initialization?\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "In the context of weight initialization in neural networks, variance refers to the spread or dispersion of values in the initial weights assigned to the neurons. It is a measure of how much the values deviate from the mean. Properly managing the variance during weight initialization is crucial for several reasons:\n",
    "\n",
    "1. **Impact on Activation Outputs:**\n",
    "   - The weights in a neural network contribute to the activations of neurons in subsequent layers. If the variance of the weights is too small, the activations may also have a small range, potentially leading to saturation of activation functions like sigmoid or tanh. This can result in the vanishing gradient problem, where gradients become very small during backpropagation and hinder effective learning.\n",
    "\n",
    "2. **Avoiding Saturation and Vanishing Gradients:**\n",
    "   - The choice of weight initialization directly affects the range of activations. If weights are initialized with too small a variance, the activations can be squashed into a small range, causing the network to operate in the flat regions of activation functions. This can lead to vanishing gradients and slow convergence, particularly in deep networks.\n",
    "\n",
    "3. **Mitigating Exploding Gradients:**\n",
    "   - On the other hand, if the variance of weights is too large, activations can become extremely large, leading to activation functions saturating and gradients exploding during backpropagation. This instability can cause the optimization process to diverge. Managing the variance helps mitigate the risk of exploding gradients.\n",
    "\n",
    "4. **Balancing Information Flow:**\n",
    "   - Proper variance in weight initialization helps balance the flow of information through the network. It ensures that information is neither overly suppressed (due to small weights) nor excessively amplified (due to large weights), promoting a stable and effective learning process.\n",
    "\n",
    "5. **Improving Generalization:**\n",
    "   - The variance in weights also plays a role in determining the capacity of the model to generalize well to unseen data. Balanced variance helps the network learn meaningful representations from the training data, making it more likely to generalize to new, unseen examples.\n",
    "\n",
    "6. **Addressing Symmetry Issues:**\n",
    "   - Varied initial weights help break symmetry in the learning process. If all weights are initialized to the same value, neurons in a layer may end up learning the same features, limiting the expressiveness of the network.\n",
    "\n",
    "Common weight initialization techniques, such as Xavier/Glorot initialization or He initialization, are designed to manage the variance in weights effectively. These methods aim to set the initial weights in a way that considers the number of input and output units in a layer, balancing the scale of weights to prevent issues like vanishing or exploding gradients. By carefully managing the variance, practitioners can improve the stability, convergence, and generalization capabilities of neural networks during training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8e318fd-0c3a-49c5-8884-0d5cc85e9d18",
   "metadata": {},
   "outputs": [],
   "source": [
    "Part 2: Weight Initialization Techniques"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1a9918b-3fd2-4d91-9850-edfdaa5bed05",
   "metadata": {},
   "outputs": [],
   "source": [
    "4. Explain the concept of zero initialization. Discuss its potential limitations and when it can be appropriate\n",
    "to use.\n",
    "\n",
    "\n",
    "\n",
    "Zero initialization is a weight initialization strategy where all the weights in the neural network are set to zero. While this method is simple and easy to implement, it comes with several limitations and may not be the best choice in many cases. Here are some points to consider regarding zero initialization:\n",
    "\n",
    "### Limitations of Zero Initialization:\n",
    "\n",
    "1. **Symmetry Issues:**\n",
    "   - When all the weights are initialized to zero, each neuron in a layer learns the same features during training. This leads to symmetry issues, where neurons in the same layer remain identical throughout training. As a result, the network fails to capture diverse features and expressiveness, limiting its capacity to learn complex patterns.\n",
    "\n",
    "2. **Vanishing Gradients:**\n",
    "   - During backpropagation, if all weights are initialized to zero, the gradients for all neurons in a layer will be the same. This can result in the vanishing gradient problem, especially in deep networks, where the gradients become very small as they are propagated backward through the layers. This inhibits the effective learning of features in the earlier layers.\n",
    "\n",
    "3. **No Discrimination between Neurons:**\n",
    "   - With zero initialization, neurons in the same layer have identical weights, causing them to produce the same output. This lack of diversity means that the neurons in a layer cannot specialize in learning different aspects of the data.\n",
    "\n",
    "### When Zero Initialization Can Be Appropriate:\n",
    "\n",
    "While zero initialization is generally not recommended due to its limitations, there are specific scenarios where it might be appropriate:\n",
    "\n",
    "1. **Bias Initialization:**\n",
    "   - It is common to initialize bias terms to zero. Biases represent the intercept term in a neuron's output, and setting them to zero initially doesn't introduce the same symmetry and vanishing gradient issues as initializing weights to zero.\n",
    "\n",
    "2. **Transfer Learning:**\n",
    "   - In transfer learning scenarios, where a pre-trained model is fine-tuned on a new task, some layers or specific weights may be initialized to zero if they are not expected to contribute significantly to the new task. However, this is a targeted and selective use of zero initialization.\n",
    "\n",
    "3. **Sparse Initialization:**\n",
    "   - For certain sparse autoencoder architectures, zero initialization can be used for a subset of weights to encourage sparsity in the learned representations.\n",
    "\n",
    "In most cases, it is advisable to use more sophisticated weight initialization techniques, such as Xavier/Glorot initialization or He initialization, which are designed to address the challenges associated with zero initialization. These methods take into account the architecture of the network and help mitigate issues like vanishing/exploding gradients and symmetry problems, promoting more effective and stable training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2b60dca-993e-43d8-972f-7bbef72af174",
   "metadata": {},
   "outputs": [],
   "source": [
    "5. Describe the process of random initialization. How can random initialization be adjusted to mitigate\n",
    "potential issues like saturation or vanishing/exploding gradients?\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "Random initialization is a weight initialization strategy where the weights of a neural network are set to random values instead of a constant like zero. The idea is to break symmetry and ensure that each neuron starts with different initial weights, allowing them to learn different features during training. Random initialization is a common and effective practice in training neural networks. Here's how the process works and how it can be adjusted to mitigate potential issues:\n",
    "\n",
    "### Process of Random Initialization:\n",
    "\n",
    "1. **Choosing a Distribution:**\n",
    "   - Random initialization involves drawing values from a probability distribution. Common choices include a Gaussian (normal) distribution or a uniform distribution.\n",
    "\n",
    "2. **Setting the Scale:**\n",
    "   - The scale of the distribution is an important parameter. It determines how spread out or concentrated the random values are. The scale is often adjusted based on the number of input and output units in a layer.\n",
    "\n",
    "3. **Popular Random Initialization Methods:**\n",
    "   - **Gaussian Initialization:** Random values are drawn from a Gaussian distribution with a mean of 0 and a standard deviation Ïƒ. The standard deviation is often set using heuristics like \\( \\sqrt{\\frac{2}{\\text{number of input units}}} \\) or \\( \\sqrt{\\frac{2}{\\text{number of input units + number of output units}}} \\) (He initialization).\n",
    "\n",
    "   - **Uniform Initialization:** Random values are drawn from a uniform distribution within a specified range. The range is often adjusted based on the number of input and output units.\n",
    "\n",
    "### Mitigating Issues with Random Initialization:\n",
    "\n",
    "1. **Xavier/Glorot Initialization:**\n",
    "   - Xavier/Glorot initialization is a popular method that adjusts the scale of the Gaussian or uniform distribution based on the number of input and output units. For a Gaussian distribution, the standard deviation is set as \\( \\sqrt{\\frac{2}{\\text{number of input units + number of output units}}} \\). For a uniform distribution, the range is set as \\( \\sqrt{\\frac{6}{\\text{number of input units + number of output units}}} \\).\n",
    "\n",
    "2. **He Initialization:**\n",
    "   - He initialization is a variant of Xavier initialization specifically designed for rectified linear units (ReLU) activation functions. For a Gaussian distribution, the standard deviation is set as \\( \\sqrt{\\frac{2}{\\text{number of input units}}} \\). For a uniform distribution, the range is set as \\( \\sqrt{\\frac{6}{\\text{number of input units}}} \\).\n",
    "\n",
    "3. **Scaling for Activation Functions:**\n",
    "   - Adjust the scale of the random initialization based on the activation function used. For example, He initialization is recommended for ReLU activations to address the dying ReLU problem.\n",
    "\n",
    "4. **Batch Normalization:**\n",
    "   - Using batch normalization can help mitigate the impact of poor weight initialization. Batch normalization normalizes the inputs to a layer, making the network less sensitive to the choice of initialization.\n",
    "\n",
    "By carefully selecting the distribution, scale, and method of random initialization, practitioners can mitigate issues like saturation, vanishing/exploding gradients, and promote more stable and effective training in neural networks. Choosing an appropriate initialization strategy is an important consideration in building and training robust deep learning models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3246742b-445e-4c00-93a9-f2bd45cac95a",
   "metadata": {},
   "outputs": [],
   "source": [
    "6. Discuss the concept of Xavier/Glorot initialization. Explain how it addresses the challenges of improper\n",
    "weight initialization and the underlying theory behind it.\n",
    "\n",
    "\n",
    "\n",
    "Xavier/Glorot initialization is a weight initialization technique designed to address the challenges associated with improper weight initialization in neural networks, particularly in deep networks. It is named after Xavier Glorot, who introduced this method in the paper \"Understanding the difficulty of training deep feedforward neural networks.\"\n",
    "\n",
    "The goal of Xavier/Glorot initialization is to set the initial weights of the neurons in a way that helps mitigate the issues of vanishing or exploding gradients during training. The underlying theory is based on maintaining a certain variance in the activations across layers to ensure a stable and effective learning process. Here's a more detailed explanation of Xavier/Glorot initialization:\n",
    "\n",
    "### Key Concepts and Formulas:\n",
    "\n",
    "1. **Gaussian Distribution:**\n",
    "   - The weights are initialized by drawing random values from a Gaussian (normal) distribution.\n",
    "\n",
    "2. **Variance Preservation:**\n",
    "   - The key insight behind Xavier/Glorot initialization is to preserve the variance of the activations across layers during forward and backward passes. This helps in preventing the vanishing or exploding gradient problem.\n",
    "\n",
    "3. **Scaling Factor for Gaussian Distribution:**\n",
    "   - For a layer with \\(n_{\\text{in}}\\) input units and \\(n_{\\text{out}}\\) output units, the scaling factor for the Gaussian distribution is calculated as:\n",
    "     \\[\n",
    "     \\text{scale} = \\sqrt{\\frac{2}{n_{\\text{in}} + n_{\\text{out}}}}\n",
    "     \\]\n",
    "     The random weights are then drawn from the Gaussian distribution with a mean of 0 and a standard deviation of \\(\\text{scale}\\).\n",
    "\n",
    "4. **Uniform Distribution Equivalent:**\n",
    "   - If weights are drawn from a uniform distribution, the recommended range is \\(\\pm \\sqrt{\\frac{6}{n_{\\text{in}} + n_{\\text{out}}}}\\).\n",
    "\n",
    "### How Xavier/Glorot Initialization Addresses Challenges:\n",
    "\n",
    "1. **Mitigating Vanishing Gradients:**\n",
    "   - By maintaining an appropriate variance in the weights, Xavier/Glorot initialization helps prevent the vanishing gradient problem. The scale factor ensures that the weights are initialized in a way that allows gradients to flow effectively during backpropagation.\n",
    "\n",
    "2. **Preventing Exploding Gradients:**\n",
    "   - The initialization also prevents exploding gradients by ensuring that the weights are not too large. The controlled variance helps in stabilizing the learning process and prevents large updates to the weights.\n",
    "\n",
    "3. **Balancing Information Flow:**\n",
    "   - Xavier/Glorot initialization helps balance the information flow through the network by avoiding extreme values in the initial weights. This is crucial for the stability and convergence of the training process.\n",
    "\n",
    "4. **Applicability to Various Activation Functions:**\n",
    "   - Xavier/Glorot initialization is designed to be applicable to a variety of activation functions, including sigmoid, tanh, and variants of rectified linear units (ReLU).\n",
    "\n",
    "In summary, Xavier/Glorot initialization is an effective strategy for initializing weights in neural networks. Its success lies in maintaining an appropriate variance in the weights, which helps address challenges related to vanishing or exploding gradients during training. It is widely used in practice and has become a standard initialization method in many deep learning frameworks and architectures."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06120372-1abc-4b34-9529-1e0aadab792b",
   "metadata": {},
   "outputs": [],
   "source": [
    "7. Explain the concept of He initialization. How does it differ from Xavier initialization, and when is it\n",
    "preferred?\n",
    "\n",
    "\n",
    "\n",
    "He initialization, named after its author Kaiming He, is a weight initialization technique that is specifically designed to address challenges associated with the rectified linear unit (ReLU) and its variants. Like Xavier/Glorot initialization, He initialization aims to set the initial weights of neurons in a way that facilitates effective training. The key difference between He initialization and Xavier initialization lies in the scaling factor used for the Gaussian distribution.\n",
    "\n",
    "### Key Concepts and Formulas:\n",
    "\n",
    "1. **Gaussian Distribution:**\n",
    "   - Similar to Xavier/Glorot initialization, He initialization initializes weights by drawing random values from a Gaussian (normal) distribution.\n",
    "\n",
    "2. **Scaling Factor for Gaussian Distribution:**\n",
    "   - For a layer with \\(n_{\\text{in}}\\) input units, the scaling factor for the Gaussian distribution in He initialization is calculated as:\n",
    "     \\[\n",
    "     \\text{scale} = \\sqrt{\\frac{2}{n_{\\text{in}}}}\n",
    "     \\]\n",
    "     The random weights are then drawn from the Gaussian distribution with a mean of 0 and a standard deviation of \\(\\text{scale}\\).\n",
    "\n",
    "3. **Uniform Distribution Equivalent:**\n",
    "   - If weights are drawn from a uniform distribution, the recommended range is \\(\\pm \\sqrt{\\frac{6}{n_{\\text{in}}}}\\).\n",
    "\n",
    "### Differences from Xavier Initialization:\n",
    "\n",
    "1. **Scaling Factor:**\n",
    "   - The critical distinction between He initialization and Xavier initialization is the scaling factor. He initialization uses a scaling factor of \\(\\sqrt{\\frac{2}{n_{\\text{in}}}}\\), while Xavier initialization uses \\(\\sqrt{\\frac{2}{n_{\\text{in}} + n_{\\text{out}}}}\\). The difference in scaling factors is particularly significant in the context of activation functions like ReLU.\n",
    "\n",
    "2. **Activation Functions:**\n",
    "   - He initialization is specifically designed for activation functions like ReLU and its variants. ReLU activations can suffer from the \"dying ReLU\" problem, where neurons can become inactive during training. He initialization helps mitigate this issue by providing higher variance in the initial weights.\n",
    "\n",
    "### When is He Initialization Preferred:\n",
    "\n",
    "1. **ReLU Activation and Variants:**\n",
    "   - He initialization is particularly well-suited for networks that use the rectified linear unit (ReLU) activation function or its variants (e.g., Leaky ReLU, Parametric ReLU). ReLU activations can benefit from higher variances to avoid the issue of neurons becoming inactive (dying ReLU problem).\n",
    "\n",
    "2. **Deep Networks:**\n",
    "   - He initialization is often preferred in deeper networks where the vanishing gradient problem is a concern. It helps ensure that the gradients do not become too small, allowing for more effective training in deep architectures.\n",
    "\n",
    "3. **Empirical Performance:**\n",
    "   - In practice, He initialization has shown good empirical performance in training deep neural networks, especially when ReLU-based activations are used.\n",
    "\n",
    "While Xavier/Glorot initialization is a general-purpose initialization method, He initialization is a specialized technique that is tailored to the characteristics of ReLU activations. The choice between the two often depends on the specific activation functions used in the network and the nature of the problem being addressed. It is common to experiment with both and choose the initialization method that results in better training performance for a given task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72dca817-5fb7-4df0-8bb0-bdb4a04a2904",
   "metadata": {},
   "outputs": [],
   "source": [
    "Part 3: Applying Weight Initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16ee4508-483e-45d8-b3e2-c500a89a0704",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3ecb9b2-12a3-48be-a97d-26a9cbc45fa5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6666d84-4b40-498c-b368-bcefa289ddba",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b89ad5b6-89ab-4551-94ad-0753f300c9aa",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ec99765-7934-40b3-b542-9b1eba6159b5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9689f75-e89a-4c50-b3eb-b89cd44c6b58",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2eb495d-f8c5-461c-a976-29b9fb9866cd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "715c2441-177b-4b38-a12e-c9126d5d4b8e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4bea1cf-b1c0-45c2-86ae-51f91750b7f6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b6519a9-d0a6-46ab-acc8-75c0c60498bf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d06de71a-882f-4f11-bb54-c070cab0ab83",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
